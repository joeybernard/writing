File Formats Used in Science

We have looked at several topics over these articles. We've looked at specific software packages, programming libraries and algorithm designs. One subject we haven't covered yet is data storage, specifically data formats used for scientific information. To this end, we'll look at two of the most common file formats: NetCDF and HDF. Both of these file formats include command line tools and libraries that allow you to access these file formats from within your own code.

NetCDF (Network Common Data Format) is an open file format designed to be self-describing and machine independent. The project is hosted by the Unidata program at UCAR (University Corporation for Atmospheric Research). It is being actively worked on by UCAR, with version 4.1 being released in 2010. NetCDF supports 3 separate binary data formats. The classic format has been used since the very first version of NetCDF, and is still the default format. Starting with version 3.6.0, a 64-bit offset format was introduced that allowed for larger variable and file sizes. Then, starting with version 4.0, NetCDF/HDF5 was introduced which was HDF5 with some restrictions. These files are meant to be self-describing, as well. This means that they contain a header which describes in some detail all of the data that is stored in the file. The easiest way to get NetCDF is to check your distribution's package management system. But sometimes, the included version may not have the compile time settings that you need, In these cases, you will need to grab the tar ball and do a manual installation. There are interfaces for C, C++, FORTRAN 77, FORTRAN 90 and Java.

The classic format consists of a file that contains variables, dimensions and attributes. Variables are N-dimensional arrays of data. This is the actual data (i.e. numbers) that you use in your calculations. This data can be one of six types (char, byte, short, int, float and double). Dimensions describe the axes of the data arrays. A dimension has a name and a length. Multiple variables can use the same dimension, indicating that they were measured on the same grid. At most one dimension can be unlimited, meaning that the length can be continually updated as more data is added. Attributes allow you to store metadata about the file or variables. They can be either scalar values or 1 dimensional arrays. A new, enhanced format was introduced with NetCDF 4. To remain backwards compatible, it is constructed from the classic format plus some extra bits. One of the extra bits is the introduction of groups. Groups are hierarchical structures of data, similar to the Unix filesystem. The second extra part is the ability to define new data types. A NetCDF 4 file contains one top-level unnamed group. Every group can contain one or more named subgroups, user-defined types, variables, dimensions and attributes.

There are some standard command line utilities available to allow you to work with your NetCDF files. The utility ncdump takes the binary NetCDF file outputs a text file in a format called CDL. The utility ncgen takes a CDL text file and creates a binary NetCDF file. nccopy copies a NetCDF file, and in the process allows you to change things like the binary format, chunk sizes, and compression. There is also the NetCDF Operators, or NCO. This project consists of a number of small utilities that do some operation on a NetCDF file. These operations could be concatenation, averaging or interpolation.

A simple example of a CDL file is given below:
   netcdf simple_xy {
   dimensions:
      x = 6 ;
      y = 12 ;
   variables:
      int data(x, y) ;
   data:
     
    data =
     0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
     12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,
     24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
     36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,
     48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,
     60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71 ;
   }
Once you have this defined, you can create the corresponding NetCDF file with the utility ncgen.

To use the library, you will need to include the header file "netcdf.h". The library function names start with "nc_". To open a file, you need to use "nc_open(filename, access_mode, file_pointer)". This gives you a file pointer that you can use to read from and write to the file. You then need to get a variable identifier with the function "nc_inq_varid(file_pointer, variable_name, variable_identifier)". You can now actually read in the data with the function "nc_get_var_int(file_pointer, variable_identifier, data_buffer)", which will place the data into the data buffer in your code. When you're done, you need to close the file with "nc_close(file_pointer)". All of these functions return error codes, and they should be checked after each execution of a library function. Writing files is a little different. You need to start with nc_create, which gives you a file pointer. You then need to define the dimensions with the function nc_def_dim. Once these are all defined, you can go ahead and create the variables with the function nc_def_var. You will need to close off the header with nc_enddef. Finally, you can start to write out the data itself with nc_put_var_int. Once all of the data is written out, you can close the file with nc_close.

The Hierarchical Data Format (HDF) is another very common file format used in scientific data processing. It was originally developed at the National Center for Supercomputing Applications, and is now maintained by the non-profit HDF Group. All of the libaraies and utilities are released under a BSD-like license. There are two options available, HDF4 and HDF5. HDF4 supports things like multidimensional arrays, raster images and tables. You can also create your own grouping structures called vgroups. The biggest limitation to HDF4 is that file size is limited to a 2GB maximum. There also isn't a clear object structure, which limits the kind of data that can be represented. HDF5 simplifies the file format so that there are only two types of objects: datasets, which are homogeneous multidimensional arrays, and groups, which are containers that can hold datasets or other groups. The libraries have interfaces for C, C++, FORTRAN 77, FORTRAN 90 and Java, similar to NetCDF. The file starts with a header, describing details of the file as a whole. Then it will contain at least one data descriptor block, describing the details of the data stored in the file. The file can then contain zero or more data elements, which contain the actual data itself. A data descriptor block plus a data element block is represented as a data object. A data descriptor is 12 bytes long, made up of a 16-bit tag, a 16-bit reference number, a 32-bit data offset and a 32-bit data length.

There are several command line utilities available for HDF files, too. The utility hdp is like the utility ncdump. It gives a text dumping of the file and it's data values. hdiff gives you a listing of the differences between two HDF files. hdfls shows information on the types of data objects stored in the file. hdfed displays the contents of an HDF file, and gives you limited abilities to edit the contents. You can convert back and forth between HDF4 and HDF5 with the utilities h4toh5 and h5toh4. If you need to compress the data, you can use the program hdfpack. If you need to alter options like compression or chunking, you can use hrepack.

The library API for HDF is a bit more complex than NetCDF. There is a low-level interface which is similar to what you would see with NetCDF. Built on top of this is a whole suite of different interfaces that give you higher level functions. For example, there is the scientific data sets interface, or SD. This provides functions for reading and writing data arrays. All of the functions begin with SD, such as SDcreate to create a new file. There are many other interfaces, such as for palettes (DFP) or 8-bit raster images (DFR8). There are far too many to do more than introduce them to you. But there is a great deal of information, including tutorials, that can help you get up to speed with HDF.

Hopefully, now that you have seen these two file formats you can start to use them in your own research. The key to expanding scientific understanding is the free exchange of information. And in this age, that means using common file formats that everyone can use. So now you can go out and set your data free, too.


URL's
NetCDF - http://www.unidata.ucar.edu/software/netcdf/
HDF - http://www.hdfgroup.org/
