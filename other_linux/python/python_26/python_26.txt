@ Title (2-4 words)

Motion Tracking with Your Raspberry Pi



@ Standfirst (20 words approx) - 19 words

This month, you will learn how to track motions with your Raspberry Pi, a camera and some Python code.



@ Bio (20-30 words)

Joey Bernard is a true Renaissance man, splitting his time between building furniture, helping researchers with scientific computing problems and writing Android apps



@ Body ( = 9400 - P characters, where P = [no. of lines of code in new paragraphs x 35] + [no. of new paragraphs for code x 70], eg 5 lines of code at 35 characters per line, split into 2 separate paragraphs rather than a single paragraph of five lines, means a reduction of 315 characters from the overall character count) - 9220 characters

In a previous article, we looked at how you can capture images using a camera and a Raspberry Pi. This let you include image capture functionality within your own Python program. But, there is so much more you can do once you add vision to your code. This month, we will look at how you can add motion detection to your Python program. This kind of advanced image processing is extremely difficult to do, so we will definitely be building on the hard work of others. Specifically, we will be using the OpenCV Python package. This package is constantly being improved, with much more functionality being added with every update.

The first thing you will need to do is to install the various Python packages that you will need to talk to the camera and use OpenCV. Installing the packages can be done with
   sudo apt-get install python-picamera python-opencv
This will also install all of the required dependencies. This project will assume that you will use the Camera Module for the Raspberry Pi. Checkout the box out section for other options if you wanted to try using a USB webcam. To talk to the camera module, you need to import the PiCamera class from the picamera Python module. You will also need the PiRGBArray class so that you can the raw data from the camera. To talk to the camera, you instantiate a new instance of the PiCamera class. You can then set the resolution and framerate before you start capturing images.
   from picamera import PiCamera
   from picamera import PiRGBArray
   camera = PiCamera()
   camera.resolution = tuple([640,480])
   camera.framerate = 16
   rawImage = PiRGBArray(camera, tuple([640,480]))
You now have your camera ready, and a memory buffer available to store the captured images in.

There are several different methods that you can use to do motion tracking. One of the simpler ones is try and notice when something within the image field changes. There is a Python module, called imutils, that provides several basic image processing functions that are useful in the pre-processing steps. There is no package for it within Raspbian, however, so you will want to install it with
   sudo pip install imutils
To look at image changes, we need to see what the background image looks like. You can take a series of images and look at the average of them to get an idea of the general background. Then, if a new image differs from the averaged background, we know that something has changed. This change is most probably due to something moving within the field of the image. To simplify the process, we will gray scale the image and then blur it slightly to get rid of any high contrast regions. You will then want to simply run a continuous loop, pulling an image from the camera and running this process:
   import imutils
   import cv2
   for f in camera.capture_continuous(rawImage, format='bgr', use_video_port=True):
      frame = imutils.resize(f.array, width=500)
      gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
      gray = cv2.GaussianBlur(gray, (21, 21), 0)
Here we start using the OpenCV functions to handle the image processing steps. You may have noticed that we are actually working with the array representation of the raw image data for the captured frame. There is no meta-data wrapping this image information, so it is your responsibility to remember what you are working with. The next step within the loop is to check whether we have an averaged image yet, and to initialize it if we don't. So the first time through the loop, the following code will execute
      if avg is None:
         avg = gray.copy().astype("float")
         rawImage.truncate(0)
         continue
Now that we have an averaged image, we can add every subsequent captured image to the weighted average. We also need to find how different the current image is from this weighted average.
      cv2.accumulateWeighted(gray, avg, 0.5)
      imgDiff = cv2.absdiff(gray, cv2.convertScaleAbs(avg))
By using this weighted average, we should be able to deal with false positive hits due to environment changes like changes in lighting.

Now that you have what is different from the average, what can you do with it? How do you decide how different it is from the average? We need to set some threshold difference that signifies a "real" difference in the image from the average. If you then dilate this thresholded image, you can apply the 'findContours' function to identify the contours of the objects that are different from the averaged background.
      imgThresh = cv2.threshold(imgDiff, 5, 255, cv2.THRESH_BINARY)[1]
      imgThresh = cv2.dilate(imgThresh, None, iterations=2)
      (conts, _) = cv2.findContours(imgThresh.copy(), cv2.RETR_EXTERNAL,
                                                                 cv2.CHAIN_APPROX_SIMPLE)
This dumps all of the contours from the current image into the list 'conts'. You probably aren't very interested in tiny objects within the list of contours. These might simply be artifacts within the image data. You should loop through each of these and ignore any that are below some area limit. You probably want to highlight any remaining object contours by placing a bounding box around them. Luckily, OpenCV provides a function that will give the corner coordinates and the width and height. You can then draw a box on the image using this information.
      for c in conts:
         if cv2.contourArea(c) < 5000:
            continue
         (x, y, w, h) = cv2.boundingRect(c)
         cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
You should now have an image with all of the moving objects highlighted by red bounding boxes.

What can you do with these annotated images, though? If you have a graphical environment available, you can display these results directly on the screen. OpenCV includes several functions to display the results of your image analysis. The simplest is to use the "imshow()", which will pop up a window to display the image and also add a title.
      cv2.imshow("Motion detected", frame)
If you aren't monitoring the results of your motion detector in real time, you probably still want to capture when something moves in the environment. Luckily, OpenCV also includes a pretty exhaustive list of IO functions. You will probably want to timestamp these images first, though. Using the Python module timestamp and the function "putText()", you can get the current time and date and add it to the image itself with
      import timestamp
      ts = timestamp.strftime("%A %d %B %Y %I:%M:%S%p")
      cv2.putText(frame, ts, (10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX,
                                0.35, (0, 0, 255), 1)
Now you have an image with the current time and date on it, and the parts of the image that show up as having movement bounded in red boxes. You can use the OpenCV IO functions to write out these images so that you can check them out later. The following code is an example:
      cv2.imwrite("filename.jpg", frame)
The function "imwrite()" uses the file name extension to figure out what format to use when writing out the image. It can handle JPEG, PNG, PBM, PGM, PPM and TIFF. If the particular format you want to use also takes options, you can include them in the call to "imwrite()". For example, you can set the JPEG quality by including CV_IMWRITE_JPEG_QUALITY and setting it to some value between 0 and 100.

Everything we have looked at has been focused on the idea of analyzing the images in real time. This is great if you can put the Raspberry Pi in the same location as the camera. If you can't fit it in, though, you can still use the above ideas to post-process the video recorded by your micro-camera. You can use the same OpenCV IO functions to load the video file with
   camera = cv2.VideoCapture("filename.avi")
You can then run through the same process to analyze each of the image frames within the video file. The "VideoCapture()" function can also read in a series of image files if your camera is simply grabbing a series of still images rather than a video.

Once your program finishes, you need to remember to clean up after yourself. You should release the camera that you were using, and if you had OpenCV display any images on the desktop, you should clean those up, too.
   camera.release()
   cv2.destroyAllWindows()
You should have enough information now to be able to add some basic motion detection to your own Python programs. If you explore the OpenCV documentation, you will find many other, more complex, image processing and analyzing tools that are available to play with. Also, more functionality is constantly being added to the OpenCV project.



@ Boxout title (2-4 words) - 3 words

What About Webcams?



@ Boxout text ( = 2200 - Q characters, where Q = [no. of lines of code in new paragraphs x 45] + [no. of new paragraphs for code x 90] ) - 2253 characters

In the main article, we have been using the Raspberry Pi Module that plugs into the IO bus of the Pi. But, what if you don't have easy access to one of these? Almost everyone has an old webcam sitting around the house somewhere, and the Raspberry Pi  has a perfectly useful USB port. The image quality and frame per second count is not as good as what you can get with the actual Pi Module. The key is getting the image data off the camera in the format that the OpenCV image analysis functions is expecting. The "VideoCapture()" function can not only take a video file name to read in, but can also take device IDs for cameras attached to the Raspberry Pi. Assuming that you only have one camera attached, you can connect to it with
   camera = cv2.VideoCapture(0)
Making sure that your USB webcam is correctly connected and that Linux can properly talk to it is always the place where you may run into issues. But, if everything works the way it should, you can use all of the ideas from the main body of the article to use it for motion detection. While OpenCV has some capabilities to interact with the user, you may want to use some other framework to handle this. A good framework that is also very fast is pygame. You can use OpenCV to handle all of the image processing steps and build your user interface with pygame. The only issue is that the internal formats used by OpenCV and pygame to store image data are different, so you will need to do a translation back and forth. You only really need to worry about translating from OpenCV to pygame, since that is the direction that information will flow. There are a few helper functions that you can use to convert the OpenCV image to a string format, and then a pygame function to import this string into a pygame image. As an example, you could use something like
   pygameImg = pygame.image.frombuffer(cv2Img.tostring(), cv2Img.shape[1::-1], "RGB")   
This takes images from OpenCV (stored in cv2Img) into a pygame format (stored in pygameImg). If you have to, you can do a similar transformation using strings back from pygame to OpenCV format.



@ Full code listing (optional, no more than 50 lines of code at 70 characters per line; if this is supplied, change the body text to (6100 - P) characters)
xxxx
