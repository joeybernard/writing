@Title
Adding Vision to Your Raspberry Pi



@standfirst - 16 words
The Kinect can provide a very robust vision system to your Raspberry Pi, all through Python.



@body - 1012 words
If you do even a cursory search online, you will be able to find any number of modules that will add functionality to your Raspberry Pi. Many of these are sensor modules that let you observe the world around you. What you may not know is that you probably already have a very powerful sensor module that you can use with your Raspberry Pi if you own an XBox, the Kinect sensor. The Kinect connects through a regular USB cable, so theoretically it could be connected to your Raspberry Pi. All you need is a library to establish communication. Luckily, this is provided through the OpenKinect project (http://www.openkinect.org). If you use Raspbian, then you should have everything you need available in the package repository. The base libraries are available with the command
   sudo apt-get install freenect
If you were to write C code, you could stop here. Since we are focussing on Python, you will also need to install the Python wrappers. You do this with the command
   sudo apt-get install python-freenect
The last step to getting set up is to connect the Kinect to your Raspberry Pi. The USB ports on the Raspberry Pi probably can't provide enough power for the Kinect, so you will need a powered USB hub placed between the two devices.

Now that everything is connected together, we will look at some Python code to start communicating with the sensor. We will be borrowing heavily from the demos included in the Python wrapper source code. The first step is to import the freenect module, with
   import freenect
Be sure that everything is connected and powered up before doing this. You should now have access to your kinect. The first thing you can do, just to verify that you have control of your Kinect, is to set the LED value. You can set it to green, red, yellow, blinking or off, with the function
   freenect.set_led(dev, led)
where 'led' is an integer from 0 to 6 representing the various options. The parameter 'dev' contains a pointer to the Kinect device. You can also tilt the Kinect by using the function 'set_tilt_degs(dev, tilt_degrees)'. Just be sure to keep it between 0 and 30 degrees. In order to get the device parameter, you need to get the freenect module to tell you what it is. One way to do this is to use the 'runloop' wrapper function. This function takes a function you wrote as the body, initializes the Kinect, and runs the body in an infinite loop. But what if you want to break out of the loop? You can use a global variable to trigger what to do. But how can you set it? Here, we'll need to introduce signals. Signals are messages that are sent to your program by the kernel. For example, if you enter CTRL-C, this sends the signal SIGINT, or the interrupt signal, to your program. Other examples are SIGKILL (the kill signal), or SIGTERM (the terminate signal). In Python, you can work with signals by importing the signal module. You can then set up a handler to listen for particular signals and do something when you get one. In the example code, we have a global variable named 'keep_running' that tells freenect whether to continue in the 'runloop' function or not. You can create a handler function that looks like the following
   def handler(signum, frame):
      global keep_running
      keep_running = False
Once you have this handler function, you can register it to be fired when a particular signal is received. So, if you wanted to fire this when the user types CTRL-C, you would use
   signal.signal(signal.SIGINT, handler)

Now we should start trying to get some data from the Kinect. There are both synchronous and asynchronous versions of the get functions to get data from the Kinect. The asynchronous versions require callback functions, so we will skip these versions for now. The synchronous versions block on each call until the data is returned. On the return, you will get a list of objects, and you will be interested in the first element. This looks like
   freenect.sync_get_video()[0]
The simplest thing to do is to display this video data. Since we are in Python, we can use matplotlib and get an animated display. A simplified loop looks like
   image_rgb = matplotlib.imshow(get_video(), interpolation='nearest', animated=True)
   while keep_running:
      image_rgb.set_data(get_video())
      matplotlib.draw()
where the function 'get_video()' wraps the call to freenect's 'sync_get_video'. Again, we can use the signal handler we defined above to be able to let the user break out of this infinite loop. Things get more interesting when you start to analyze all of this video data. The OpenCV project provides tools to do some pretty serious data analysis. In order to do this, you will need to do some data conversion first. The main difference is that the data from the Kinect is in RGB order, while OpenCV is expecting it to be in BGR order. Once you make this conversion, you can reproduce the display from above with
   import cv
   cv.NamedWindow('Video')
   while 1:
      cv.ShowImage('Video', get_video())
      if cv.WaitKey(10) == 27:
         break
where 'get_video()' is a wrapper that takes care of the conversion for OpenCV. Along with the video information, there are also equivalent functions to get depth information. The last source of information available from the Kinect is accelerometer data. With this data, you can figure out the orientation of the Kinect, to help you map out where the video data is located relative to the Kinect sensor. The 'get_accel()' function returns a set of x, y and z values for the x, y and z parts of the local acceleration. With all of this data, you can start some pretty interesting projects.

There is not a lot of documentation available on how to use the Python wrapper for freenect, so you will need to poke around a bit to discover what you can do. When you learn something new, be generous and share it with others so that we can all improve. There is a lot you can do with the combination of a Kinect with your Raspberry Pi.



@boxout - 198 words
One project that might come to mind that uses a Kinect and a Raspberry Pi is a vision system for a robot. You could do some pretty amazing stuff with this. But, if you wanted to do even more extensive work, you should look at the robot operating system (http://www.ros.org). This is a project that collects a full suite of tools, libraries and conventions to provide a full platform to develop your own robot. It is built on Ubuntu as the default distribution, and is what is expected if you install it on your desktop or laptop. But, for you Raspberry Pi people out there, there is a port of the version Groovy, called ROSberryPi. You will need to have Raspbian installed as the OS on your Raspberry Pi, and then you can add an APT source pointing to ROS. Then you can install it with
   sudo apt-get install ros-groovy-ros-com
After that, there will be some environment variables to setup, which is handled by a shell script included with ROS. Then you can start playing by running 'roscore'. It isn't a complete port yet, but is certainly a good start to building your own Raspberry Pi powered robot.



@code - 58 lines
# Need to import the required modules
import freenect
import time
import random
import signal

# Some global variables
keep_running = True
last_time = 0

# This is the function that will be
# run as the body of a run loop in freenect
# This example will randomly change the
# LED and the tilt angle of the kinect
def body(dev, ctx):
   global last_time
   if not keep_running:
      raise freenect.Kill
   if time.time() - last_time < 3:
      return
   last_time = time.time()
   led = random.randint(0, 6)
   tilt = random.randint(0, 30)
   freenect.set_led(dev, led)
   freenect.set_tilt_degs(dev, tilt)

# We will need a signal handler to let
# the user interrupt the run loop
def handler(signum, frame):
   """Sets up the kill handler, catches SIGINT"""
   global keep_running
   keep_running = False

# Now we set the signal handler and start
# the run loop - it will stop when you press
# CTRL-C
signal.signal(signal.SIGINT, handler)
freenect.runloop(body=body)

# If we want to get video data and display
# it, we need to import matplotlib
import matplotlib.pyplot as mp
keep_running = True

# Need to setup matplotlib
mp.ion()
mp.gray()

# Get an initial frame, and setup
# the signal handler
image_rgb = mp.imshow(freenect.sync_get_video()[0], interpoloation='nearest', animated=True)
signal.signal(signal.SIGINT, handler)

# And now we loop
while keep_running:
   image_rgb.set_data(freenect.sync_get_video()[0])
   mp.draw()
   mp.waitforbuttonpress(0.01)
