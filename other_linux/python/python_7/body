A few issues ago, we looked at using MPI with Python on your Raspberry Pi. This is useful when you are dealing with large problems that you want to spread across multiple machines. But what kind of work can you do on each of these machines? If you want to create a cluster of Raspberry Pis to do scientific work, you will want to look into using scipy to handle the actual computation tasks. Scipy is built on top of numpy, a Python module that provides enhanced number handling capabilities. Numpy uses optimized shared libraries, like LAPACK or BLAS, to handle linear algebra type operations at the same speed as code written in C or FORTRAN. With these modules, we will take a quick look at what kind of science you can do with these powerful little machines.

Almost every Linux distribution, including the standard ones for the Raspberry Pi, include packages for scipy and numpy. To install scipy, you would execute
   sudo apt-get install python-scipy
Since numpy is a dependency of scipy, it will get automatically installed when you install scipy. In order to use them you will need to import them, usually with an alias, 
   import numpy as np
   import scipy as sp
This import of the main scipy module only provides some core functionality that is available. All of the rest of the functions are subdivided into a number of sub-modules, such as cluster (providing clustering algorithms), integrate (providing integration and ordinary differential equation solvers), signal (for signal processing) and weave (for C/C++ integration). Each of these sub-modules needs to be imported individually. In this way, you can just import the sections you need to solve the problem at hand without cluttering up the namespace with unneeded objects.

Before we dive into scipy, it is worth taking a look at numpy and why it is worth considering Python for scientific computing. The default extended object in Python is a list. The problem with this is that a list could contain any type of object. Numpy provides an array object that can only containg elements of a single type. With this concession, you can start to take some shortcuts because you have some meta-knowledge about the data. In regular Python, when you scale a vector you might use a for loop like
   a = [1,2,3,4]
   b = []
   scale = 2
   for curr_a in a:
      b.append(curr_a * scale)
In this small example, there is a lot of overhead involved. On each iteration around the for loop, Python needs to check the type of scale and the type of the current element from the list to see what it needs to do to apply the multiplication operation. When you use numpy arrays, all of this goes away. The equivalent code would look like
   a = np.array([1, 2, 3, 4])
   scale = 2
   b = scale * a
In this case, Python only needs to check the type of scale and a once. It then hands the two objects out to the shared linear algebra library to handle the actual multiplication step. Then the answer object comes back and is stored in b. So you get two speedups, one from removing all of the type checking that Python does and another from using optimized routines in the shared library.

Getting back to scipy, we should look at what we can do. A common task in scientific problem solving is integrating a function over some range. The usual scipy function to do this is 'integrate.quad()'. You can either hand in a predefined Python function, or create a lamda expression on the fly. As an example, say you want to integrate the sine function from 0 to 1. You could do so with
   import scipy.integrate as spi
   import math as m
   result = spi.quad(lambda x: m.sin(x), 0.0, 1.0)
Hopefully, you should get the result (0.45969769413186023, 5.103669643922839e-15). The first value is the integral, and the second value is an estimate of the absolute error in the result.

Another common task is try and fit a function to some experimental data. The usual methid is to do a least-square fitting to minimize the difference between the function and the experimental data. The function 'leastsq' is in the 'scipy.optimize' sub-module. To do this type of calculation, you need to provide a function to compute the residuals, and a starting point for each of the adjutable parameters in this function. As an example, say you had some measurements that seemed to be in a sinusoidal shape. You could try fitting a general sinusoidal function 
   y = A * sin(2*pi*k*x + theta)
to these measurements. The residual function would be
   residuals = y - A*sin(2*pi*k*x + theta)
and the adjustable parameters would be 'A', 'k' and 'theta'. 

Hopefully we have covered enough in this article to convince you to give scipy and numpy a try with any problems you may need to work on. There are hundreds of functions available to do rather complicated calculations. And you can't beat the power usage per FLOP that you can get from the Raspberry Pi.