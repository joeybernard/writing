@Title (probably something like 'The Python column'
Science on a Raspberry Pi




@standfirst 14 words
This month learn how to do some science number crunching on your Raspberry Pi.



@body 1019 words
A few issues ago, we looked at using MPI with Python on your Raspberry Pi. This is useful when you are dealing with large problems that you want to spread across multiple machines. But what kind of work can you do on each of these machines? If you want to create a cluster of Raspberry Pis to do scientific work, you will want to look into using scipy to handle the actual computation tasks. Scipy is built on top of numpy, a Python module that provides enhanced number handling capabilities. Numpy uses optimized shared libraries, like LAPACK or BLAS, to handle linear algebra type operations at the same speed as code written in C or FORTRAN. With these modules, we will take a quick look at what kind of science you can do with these powerful little machines.

Almost every Linux distribution, including the standard ones for the Raspberry Pi, include packages for scipy and numpy. To install scipy, you would execute
   sudo apt-get install python-scipy
Since numpy is a dependency of scipy, it will get automatically installed when you install scipy. In order to use them you will need to import them, usually with an alias, 
   import numpy as np
   import scipy as sp
This import of the main scipy module only provides some core functionality that is available. All of the rest of the functions are subdivided into a number of sub-modules, such as cluster (providing clustering algorithms), integrate (providing integration and ordinary differential equation solvers), signal (for signal processing) and weave (for C/C++ integration). Each of these sub-modules needs to be imported individually. In this way, you can just import the sections you need to solve the problem at hand without cluttering up the name-space with unneeded objects.

Before we dive into scipy, it is worth taking a look at numpy and why it is worth considering Python for scientific computing. The default extended object in Python is a list. The problem with this is that a list could contain any type of object. Numpy provides an array object that can only containing elements of a single type. With this concession, you can start to take some shortcuts because you have some meta-knowledge about the data. In regular Python, when you scale a vector you might use a for loop like
   a = [1,2,3,4]
   b = []
   scale = 2
   for curr_a in a:
      b.append(curr_a * scale)
In this small example, there is a lot of overhead involved. On each iteration around the for loop, Python needs to check the type of scale and the type of the current element from the list to see what it needs to do to apply the multiplication operation. When you use numpy arrays, all of this goes away. The equivalent code would look like
   a = np.array([1, 2, 3, 4])
   scale = 2
   b = scale * a
In this case, Python only needs to check the type of scale and a once. It then hands the two objects out to the shared linear algebra library to handle the actual multiplication step. Then the answer object comes back and is stored in b. So you get two speedups, one from removing all of the type checking that Python does and another from using optimized routines in the shared library.

Getting back to scipy, we should look at what we can do. A common task in scientific problem solving is integrating a function over some range. The usual scipy function to do this is 'integrate.quad()'. You can either hand in a predefined Python function, or create a lambda expression on the fly. As an example, say you want to integrate the sine function from 0 to 1. You could do so with
   import scipy.integrate as spi
   import math as m
   result = spi.quad(lambda x: m.sin(x), 0.0, 1.0)
Hopefully, you should get the result (0.45969769413186023, 5.103669643922839e-15). The first value is the integral, and the second value is an estimate of the absolute error in the result.

Another common task is try and fit a function to some experimental data. The usual method is to do a least-square fitting to minimize the difference between the function and the experimental data. The function 'leastsq' is in the 'scipy.optimize' sub-module. To do this type of calculation, you need to provide a function to compute the residuals, and a starting point for each of the adjustable parameters in this function. As an example, say you had some measurements that seemed to be in a sinusoidal shape. You could try fitting a general sinusoidal function 
   y = A * sin(2*pi*k*x + theta)
to these measurements. The mathematical residual function would be
   residuals = y - A*sin(2*pi*k*x + theta)
and the adjustable parameters would be 'A', 'k' and 'theta'. In order to use the 'leastsq' Python function, you would need to write this residual equation as a Python function that does the relevant calculation. Then you can hand in the function, an array with an initial guess for the parameters, and any arguments needed for the residual function to the function 'optimize.leastsq()'.

The last example we will look at is solving linear systems of equations. This is something done by many disciplines, like physics and engineering. You can do this by using the linear algebra module (linalg). Using matrix notation, the system is defined by 'Ax=b', where 'A' is the matrix of coefficients, 'x' is the vector of variables and 'b' is the vector containing the right-hand sides of each equation. If you have an 'A' and 'b' defined, you can solve this problem by using
   import scipy.linalg as sla
   x = sla.solve(A,b)
Since it uses the new data-types provided by numpy, it simplifies the code you need to write, making it easier to understand later what you were trying to write.

Hopefully we have covered enough in this article to convince you to give scipy and numpy a try with any problems you may need to work on. There are hundreds of functions available to do rather complicated calculations. And you can't beat the power usage per FLOP that you can get from the Raspberry Pi.




@boxout - 222 words
A module that is used along with scipy and numpy is matplotlib. The first two modules handle all of the number crunching tasks, while matplotlib provides a set of graphical routines to handle the displaying of all of this data and results. On your Raspberry Pi, you can install it with the command
   sudo apt-get install python-matplotlib
You can then import the module with
   import pylab as pl
The plotting functions are grouped into two broad categories, low-level and high-level. Most of the functions you will want to use are of the high-level ones. The most basic thing you will want to do is to plot some data. If you have a vector of numbers stored in the variable y, you can plot them with
   pl.plot(y)
   pl.show()
The vector is assumed to have a set of y values, where the x values are assumed to be the indexes of the elements. If you have a set of x and y points, you can plot them with 
   pl.plot(x, y)
These high-level functions will make some sensible decisions for the details of your plot with respect to items like the axes, labels, colours, etc. With the low-level functions, you take control of these options and set them explicitly. If you have used the plotting functionality in R, it is very similar to that system.




@code 60 lines
# The start of all scientific code should
# import numpy and scipy
import numpy as np
import scipy as sp


# The old way of multiplying a vector
# by a scale factor is by using
# a for loop
a = [1, 2, 3, 4]
b = []
scale = 2
for curr_a in a:
   b.append(curr_a * scale)


#The cleaner, optimized way is by
# using the numpy array data-type
a = np.array([1, 2, 3, 4])
scale = 2
b = scale * a


# To integrate sin(x) over 0 to 1,
# we need to import the integrate
# sub-module and use the quad function
import scipy.integrate as spi
import math as m
# Result will contain both the answer
# and an estimate of the error in the
# answer
result = spi.quad(lambda x: m.sin(x), 0.0, 1.0)


# This leastsq example is from
# the scipy docs
x = np.arange(0, 6e-2, 6e-2/30)
A, k, theta = 10, 1.0/3e-2, pi/6
# For testing, we will add noise to the true
# values for y
y_true = A * np.sin(2*pi*k*x + theta)
y_meas = y_true + 2*np.random.randn(len(x))
def residuals(p, y, x):
   A, k, theta = p
   err = y - A*np.sin(2*pi*k*x + theta)
   return err
p0 = [8, 1/2.3e-2, pi/3]
from scipy.optimize import leastsq
plsq = leastsq(residuals, p0, args=(y_meas, x))


# To solve systems of equations
# need to import the linalg sub-module
import scipy.linalg as spl
# A will be the matrix of coefficients
A = np.array([[1,2],[3,4]])
# b will be the vector of right-hand 
# side values
b = np.array([[5],[6]])
# We can find the values of x with 'solve'
x = spl.solve(a, b)
