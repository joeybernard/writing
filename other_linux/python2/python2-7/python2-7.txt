Pi code-based tutorial, 4 pages
-feel free to include commands and small segments of code in-line with the body text, but for larger chunks of code please use figure references or refer people to the coverdisc as we can supply them with code, if you send it in with your commission.
Note: please indicate where code starts and ends with /c/ and any output with /o/, as it enables a non-tech savvy art person to layout the pages e.g.
/c/
$ telnet 192.168.1.200 8000
/c/
/o/
Trying 192.168.1.200...
/o/

Code takes up a lot of space, so if you decide to separate out any code onto a line on its own you will need to REMOVE words from the total word count for the section your writing. This works out as follows:
One line of code is equal to 30 words (we have a line break before and after code that is being pulled out of the body copy, which is why it takes up more space), two lines of code are equal to 40 words and three lines are equal to 50 words and so on.

Note: We've included the number of characters as an extra useful reference for each section as sometimes a word count can be less accurate depending on the length of words that are used.

@ Title  -  9 words
Using ipyparallel to put your Raspberry Pis to work



@ Standfirst  -  30 words
There are lots of methods to run parallel code on your network of Raspberry Pis. This month, we will look at how to use ipyparallel to run parallel Python code.



@ Profile  -  44 words
[Joey Bernard]
Joey Bernard has written a column on scientific software, as well as a column on writing Python code for the Raspberry Pi. In his day job, he helps researchers and students at the university level in designing and running HPC projects on supercomputing clusters.



@ Resources
List everything required for this tutorial and provide URLs if applicable.
If you are using hardware, mention it and a possible stockist. For software, please provide a URL.

@ Lead image
Supply an engaging main image to illustrate the feature (grey box in the example PDF); either an interesting, representative and uncropped screenshot of the software, or if there is nothing appropriate then either round up all of the relevant icons or suggest a possible illustration or diagram recreation and we will create some artwork in-house.

NOTE: this compulsory as you can see from the PDF this is large on the first page. If you have an idea for an illustration or a hardware shot, please contact us as early as you can to allow us to get that organised.

Total word count is 2,952 (roughly 16,940 char).


@ Intro text  -  185 words
Raspberry Pi provides a very powerful, yet very energy efficient, computing package. While its portability has made a great platform for mobile applications, there is a lot of "real" computing that can be done on them as well. This month, we will look at how you can run parallel Python code on a network of Raspberry Pis. Specifically, we will look at setting up a cluster of Raspberry Pis with ipyparallel, and how to run prallel code on this cluster.

You will want to start by physically connecting the Raspberry Pis together over a network. This may be a small local network, where they are all connected to same router or switch. In all of the examples in the rest of this article, I will be assuming that the Raspberry Pis all have static IP addresses in the range of 192.168.0.1 to 192.168.0.255, since these are reserved for internal local networks. From here, we will move on to setting up the software and actually running jobs. You should end up with probably the quietest and most energy efficient supercomputing.



@ Body text
2,752 words (rouhgly 15,800 chars)
Use 8 subheadings (or crossheads) to break this text up.
@ Subheading 1  -  Introduction
Ipyparallel is a Python module that adds the ability to to Ipython run multiple Ipython engines and have them talk to each other. These could be on a single machine, but in order to maximize the amount of computing you can do, they can be run on multiple separate machines. Ipyparallel is built on a layered structure of three main parts: a controller, one or more clients and one or more engines. The engines are the IPython kernels that actually run your Python code. In our setup here, these engines will be distributed across all of the connected Raspberry Pis. The clients are the Python programs that connect to your ipyparallel cluster and request for computations to be done. This is where you actually define the tasks that need to be done and fire them off. The controller is the core of your ipyparallel cluster. It is actually made up of a hub and one or more schedulers. The hub manages the entire cluster and all of the communications between all of the parts. The schedulers wrap the engines and manage the amount of work being given to them.

@ Subheading 2  -  Installation
The first step is to install ipyparallel on your Raspberry Pi. It is not likely to be in the package repository of whatever distribution you are using. This means that in most cases, you will need to install it using pip. This also means that you will need to install pip first. In Debian-based distributions, like Raspbian, you can install it with the following command.
/c/
sudo apt-get install python-pip
/c/
If you are using Python 3.X, you will want to install 'python3-pip' instead. Once pip installed, you can install ipyparallel with the following command.
/c/
sudo pip install ipyparallel
/c/
If you are using Python 2.X, you may run into an issue here. The latest versions of ipyparallel don't support older versions of Python 2.X and 3.X, and you are more likely to have an older version of Python 2.X. This is because most projects are moving to Python 3.X. If you run into this problem, your two choices are either to move to Python 3.X or install an updated Python 2.X. That will depend on the other requirements of your project. While you don't necessarily need to do the same thing on every Raspberry Pi, especially the clients, it is probably the easiest way to make sure that all of the support libraries and Python modules are installed. So, for the rest of this article, I will assume that you went ahead and simply installed the full ipyparallel module on all of the Raspberry Pis being used.

@ Subheading 3  -  Setting up the master
The master Raspberry Pi node will be the main controller that will be called the master node in the rest of this article. On this master node, you need to start up the controller portion of ipyparallel so that all of the other nodes have somewhere to connect to. The program you use for this step is the 'ipcontroller' executable. One of the most important command line options to this command is '--ip=XXXX', where 'XXXX' is the IP address that the ipyparallel controller is allowed to listen on. If your Raspberry Pi cluster is completely isolated on its own private network, you can set this option to '*'. The complete command would look like the following.
/c/
ipcontroller --ip='*'
/c/
Otherwise, you should be sure to set this to the IP address of the master node. Once this has started up, you will see diagnostic messages being printed out to the terminal window where you started the controller. Some of these messages include the location of several files being created. On my machine, this happens to be '/home/jbernard/.ipython/profile_default'. One of the files, './security/ipcontroller_engine.json', will be used in the next section of this article. There are also several other files, some of which we will look into.

You should have noticed that all of these files are located under the subdirectory 'profile_default'. Ipyparallel uses profiles to allow you to have tuned configurations for different situations. You have a default one created the first time you start the ipcontroller program, and it gets used unless you say otherwise. If you have more than one use-case in mind, you can create a new profile with the following command.
/c/
ipython profile create --parallel --profile=myprofile
/c/
You can then use it by adding the command line option '--profile=myprofile'. All of the options that we have been including in the ipcontroller command can all be placed in configuration files in the profile directory. For example, the command line option '--ip=' would have the following lines placed in the file '/home/jbernard/.ipython/profile_default/ipcontroller_config.py'.
/c/
# in ipcontroller_config.py
HubFactory.ip = '*'
/c/
The documentation covers many more options that you can configure to tune your controller.

@ Subheading 4  -  Setting up the clients
The bulk of your Raspberry Pis will be setup as clients to execute all of the parallel code that you will be running. These clients need to be configured so that they know where the controller is running. They also need to be able to authenticate with the controller, so that they will be allowed to connect. This is where the file '/home/jbernard/.ipython/profile_default/security/ipcontroller_engine.json', from the master node, comes into play. You will need to copy this file to the directory '.../security/' on each of the Raspberry Pi engine nodes. Once this has been done on each engine node, you can start them with the following command.
/c/
ipengine --file=/path/to/my/ipcontroller-engine.json
/c/
The engines also use profiles, just like the controllers. You can create specific ones for special use-cases. In a similar fashion to the controller, the engines can use a configuration file. Specifically, they will use the file 'ipengine_config.py' in the profile directory. You can configure things like some command to run at start up, as in the example below.
/c/
c.IPEngineApp.startup_command = 'import numpy, scipy'
/c/
This would be a good choice if you were going to be doing scientific computations. After reading the relevant documents, you should be able to do quite a bit of tuning to the engines.

@ Subheading 5  -  Running a test
Now that your cluster is up and running, how do you test it? Of course, you need to write a "Hello World" program. You initialize your program with the following code.
/c/
import ipyparallel as ipp
c = ipp.Client()
/c/
This should work if you run it on the master node. If you are actually running on yet another machine, you will need to point to the 'ipcontroller-client.json' file as a parameter to the call to 'Client()'. You also have the option of including parameters to setup an SSH connection, if your network configuration requires you to do so. You can verify that your Client object can see all of the engines with the following.
/c/
c.ids
/c/
/o/
[0,1,2,3]
/o/
This is assuming that you have 4 engines fired up. The following code actually handles the "Hello World" part of your program.
/c/
c[:].apply_sync(lambda : "Hello World")
/c/
/o/
[ 'Hello World', 'Hello World', 'Hello World', 'Hello World' ]
/o/
As you can see, this line selects all of the IDs from the Client object c and calls 'apply_sync()' on them, where a lambda function is handed in as the function to be applied. The lambda function simply returns the string 'Hello World'. Assuming you did not run into any problems, you should be good to move on to more productive work.

@ Subheading 6  -  Running tasks  
Now that you have a functioning ipyparallel cluster, how do you put it to use? If you have done any type of parallel programming in another language, you may be used to the workflow of scattering your data, running your code and then gathering your results. The first step is to create a DirectView object that you can use. This is as simple as the following code.
/c/
dview = c[:]
/c/
For this example, we will get our cluster to calculate the first 64 powers of 10.
/c/
dview.scatter('x',range(64))
%px y = [i**10 for i in x]
/c/
/o/
Parallel execution on engines: [0, 1, 2, 3]
/o/
As you can see, we create a list of 64 integers and scatter them using the variable name 'x'. The second line uses the magic '%px' to execute the single Python command 'y = [i**10 for i in x]'. Ipyparallel has a full selection of magic commands, similar to IPython. The following code gets the results from this parallel run.
/c/
y = dview.gather('y')
print y
/c/
/o/
[0, 1, 1024, 59049, 1048576, 9765625, 60466176, 282475249, 1073741824,...]
/o/
This gathers the results from the remote 'y' variables and stores them in the local 'y' variable.

One issues with the above code is that all of the work simply divided across all of the engines. If one or more of the engines is faster than the others, it could be picking up work from the slower engines. If this is a possible outcome in your situation, you can use the load-balanced view, rather than the direct view, to manage the work being sent to the engines. You can get this view with the following code.
/c/
lview = c.load_balanced_view()
/c/
With this load-balanced view, we can duplicate the above example with the following code.
/c/
lview.block = True
result = lview.map(lambda x:x**10, range(64))
/c/
The first line tells ipyparallel to block until all of the results are calculated and returned. The second line uses the map method of the load-balanced view to get the work of calculating the powers of tens scattered to all available engines. By default, ipyparallel hands out one task at a time. In your particular algorithm, you may know that it makes more sense to hand out tasks 10 ata time to each engine. You can do this by adding the parameter 'chunksize=10' to the call to the map method.

One thing to note is what to do when you use non-blocking calls to map or apply. In these cases, you will get an 'AsyncResult' object returned. This happens before the results are done, which means that it is your responsibility to check to see when they are actually finished. A transparent way of working with the results is simply interating over them, as in the following example.
/c/
for i in enumerate(result):
    print(i)
/c/
If, instead, you want to simply check in on your progress before starting to work with the results, you can use the progress property of the AsyncResult object to see how many tasks have been completed. You can use this to take care of other pieces of work while periodically checking to see whether all of the tasks have finished or not.

@ Subheading 7  -  Connecting from jupyter
@ Subheading 8  -  Where to now?




@ Supporting images
3 images required; please indicate whether we need to crop in on a particular part of a screenshot.

If you wish to include diagrams, supply a reference copy and we will re-create this in-house.

If you are referencing sections of code, please make them as Figure 1, 2 and so on and mention them in the copy.

@ Captions
Images need captions, which need to be around 15 words each.

A note on captions:
These are a device intended to provide additional information that's not directly in the text of the tutorial or simply stating what the picture is of. A caption that effectively says 'This is X' isn't a good caption.

@ 2x boxouts
60 words each (roughly 350 chars); Needs a title up to 5 words. The contents can be a general tip/trick or piece of knowledge related to this tutorial, or you can go into greater depth on one particular aspect of it.

A note on boxouts:
Boxouts are intended to be 'access points' into the page, so they are meant to be interesting little reads in their own right that hopefully encourage someone scanning the page to dip into the whole tutorial. You can never assume that the reader is actually going to read your words, you have to use all the tricks to encourage them to read.
@ persistent json files
@ reuse

@ 2x Pullquotes
Choose a short quote, around 15 words, to pull the reader in as they flick through the pages.


[IMAGE USAGE FORM INFO: PLEASE GIVE COMPANY AND CONTACT DETAILS FOR THE IMAGES USED IN THIS FEATURE]
